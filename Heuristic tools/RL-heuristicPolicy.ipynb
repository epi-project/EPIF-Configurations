{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>The graph initialisation</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>The graph initialisation</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\"cluster1-cntx\":[\"cluster1-cntx\",\"cluster2-cntx\"], \"cluster2-cntx\":[\"cluster2-cntx\",\"cluster1-cntx\"]}#, \"cluster3-cntx\":[\"cluster3-cntx\"]}\n",
    "#Change the graph according to the current cluster configurations, so if you remove cluster 3 from the dict value of cluster 1, means that cluster 1 is not connected to 3\n",
    "\n",
    "#Make sure the latency graph matches the G graph \"who can talk to whom? Then what is the latench?\n",
    "link_latency = {\"cluster1-cntx\":{\"cluster1-cntx\":0, \"cluster2-cntx\":10},\"cluster2-cntx\":{\"cluster2-cntx\":0, \"cluster1-cntx\":10}}\n",
    "\n",
    "#Change this and initialise to reflect the simulated latency on each cluster-to-cluster link\n",
    "bf = {\"firewall\":{\"firewalls\":10, \"firewallm\":5, \"firewalll\":1}, \"encrypt\":{\"encrypts\":10, \"encryptm\":5, \"encryptl\":1}, \"decrypt\":{\"decrypts\":10, \"decryptm\":5, \"decryptl\":1}}\n",
    "#This reflects that the processing power of firewalll is the best in terms of processing delay and the worst with smaller configurations\n",
    "\n",
    "G_req = {\"firewall\":[\"encrypt\"], \"encrypt\":[\"firewall\"]}#, \"decrypt\":[\"firewall\"]}\n",
    "#This is the actual request rule that you want to be placed. Change this with each experiment to reflect the rule needed\n",
    "#MAKE SURE THAT THE REQUEST G_REQ MATCHES THE U VALUE\n",
    "\n",
    "usecase = {\"EHR:1C:E-F-D\":{\"firewall\":30,\"encrypt\": 20, \"decrypt\":10, \"SLA\":100}, \"EHR:1C:F-E-D\":{\"firewall\":40, \"encrypt\": 10, \"decrypt\": 10, \"SLA\": 100}, \"EHR:1C:E-D\":{\"encrypt\": 20, \"decrypt\":20, \"SLA\":100}, \"EHR:1C:E-F\":{\"firewall\":30,\"encrypt\": 20, \"SLA\":100}, \"EHR:1C:F\":{\"firewall\":40, \"SLA\":100}, \"EHR:10C:E-F-D\":{\"firewall\":130,\"encrypt\": 80, \"decrypt\":20, \"SLA\":100}, \"EHR:10C:F-E-D\":{\"firewall\":160, \"encrypt\": 20, \"decrypt\":20, \"SLA\":100},\"EHR:10C:E-D\":{\"encrypt\": 80, \"decrypt\":80, \"SLA\":100}, \"EHR:10C:E-F\":{\"firewall\":130,\"encrypt\": 80, \"SLA\":100}, \"EHR:10C:F\":{\"firewall\":160,\"SLA\":100}, \"STREAM:1C:E-F-D\":{\"firewall\":130,\"encrypt\": 80, \"decrypt\":25, \"SLA\":10}, \"STREAM:1C:F-E-D\":{\"firewall\":180,\"encrypt\": 25, \"decrypt\": 25, \"SLA\": 10}, \"STREAM:1C:E-D\":{\"encrypt\": 80, \"decrypt\":80, \"SLA\":10}, \"STREAM:1C:E-F\":{\"firewall\":130,\"encrypt\": 80, \"SLA\":10}, \"STREAM:1C:F\":{\"firewall\":180, \"SLA\":10}, \"STREAM:10C:E-F-D\":{\"firewall\":300,\"encrypt\": 150, \"decrypt\":40, \"SLA\":10}, \"STREAM:10C:F-E-D\":{\"firewall\":350, \"encrypt\": 40, \"decrypt\":40, \"SLA\":10},\"STREAM:10C:E-D\":{\"encrypt\": 150, \"decrypt\":150, \"SLA\":10}, \"STREAM:10C:E-F\":{\"firewall\":300,\"encrypt\": 150, \"SLA\":10}, \"STREAM:10C:F\":{\"firewall\":350,\"SLA\":10}, \"ALGO:1C:E-F-D\":{\"firewall\":30,\"encrypt\": 20, \"decrypt\":10, \"SLA\":100}, \"ALGO:1C:F-E-D\":{\"firewall\":40,\"encrypt\": 10, \"decrypt\": 10, \"SLA\": 100}, \"ALGO:1C:E-D\":{\"encrypt\": 20, \"decrypt\":20, \"SLA\":100}, \"ALGO:1C:E-F\":{\"firewall\":30,\"encrypt\": 20, \"SLA\":100}, \"ALGO:1C:F\":{\"firewall\":40, \"SLA\":100}, \"ALGO:10C:E-F-D\":{\"firewall\":130,\"encrypt\": 80, \"decrypt\":20, \"SLA\":100}, \"ALGO:10C:F-E-D\":{\"firewall\":160, \"encrypt\": 20, \"decrypt\":20, \"SLA\":100},\"ALGO:10C:E-D\":{\"encrypt\": 80, \"decrypt\":80, \"SLA\":100}, \"ALGO:10C:E-F\":{\"firewall\":130,\"encrypt\": 80, \"SLA\":100}, \"ALGO:10C:F\":{\"firewall\":160,\"SLA\":100}}\n",
    "#Do not change this, these are the profiled values of CPU average running different use cases and different rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate different usecases under different requests \n",
    "\n",
    "G_req1 = {\"encrypt\":[\"firewall\"], \"firewall\":[\"encrypt\", \"decrypt\"], \"decrypt\":[\"firewall\"]}\n",
    "G_req2 = {\"firewall\":[\"encrypt\"], \"encrypt\":[\"firewall\", \"decrypt\"], \"decrypt\":[\"encrypt\"]}\n",
    "G_req3 = {\"encrypt\":[\"decrypt\"], \"decrypt\":[\"encrypt\"]}\n",
    "G_req4 = {\"encrypt\":[\"firewall\"], \"firewall\":[\"encrypt\"]}\n",
    "G_req5 = {\"firewall\":[\"firewall\"]}\n",
    "\n",
    "#Specify by initialising G_req = G_reqX\n",
    "#You also need specify use case by initialising value u as an example, u= \"EHR:1C:E-F-D\"\n",
    "#Key values starting with EHR refers to usecase A\n",
    "#Keys starting with STREAM use case B\n",
    "#Keys starting with ALGO refers to usecase C\n",
    "#Keys represeted as \"USECASE:NUMBEROFCLIENTS:RULE\"\n",
    "#Make sure while initialising a use case to match it to the requested chain\n",
    "#Ex: \n",
    "u = \"EHR:10C:F\"\n",
    "G_req=G_req5\n",
    "#We have one proxy so leave this proxy value as it is, unless the proxy service you have has a different name\n",
    "proxy=\"proxy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>The graph illustration</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>The graph illustration</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "def draw(G):\n",
    "    infra = nx.DiGraph()\n",
    " #   for cl in range(len(G)):\n",
    " #       i = list(G.keys())[cl]\n",
    " #       infra.add_node(i)\n",
    "    for cl in range(len(G)):\n",
    "        i = list(G.keys())[cl]\n",
    "        for link in range(len(G[i])):\n",
    "            infra.add_edges_from([(i, G[i][link])], weight=link_latency[i][G[i][link]])\n",
    "    edge_labels=dict([((u,v,),d['weight'])\n",
    "                 for u,v,d in infra.edges(data=True)])\n",
    "\n",
    "\n",
    "    pos = nx.spring_layout(infra)\n",
    "    nx.draw_networkx_edge_labels(infra,pos,edge_labels=edge_labels)\n",
    "    nx.draw_networkx_labels(infra, pos)\n",
    "    #plt.figure(figsize=(8,8))\n",
    "    \n",
    "    nx.draw(infra, pos=pos, with_labels=True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of running pods of a microservice on each cluster\n",
    "import os\n",
    "def runningPods(cluster, app_name):\n",
    "    output = os.popen('sudo kubectl top pod --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    numPods = 0\n",
    "    which_app = len(app_name)\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if len(items[0]) > 8 and items[0][:which_app] == app_name:\n",
    "            numPods += 1\n",
    "    return numPods\n",
    "runningPods(\"cluster1-cntx\",\"firewall\"+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The utilised CPU of a microservice running on a cluster\n",
    "import os\n",
    "def getchar(string, n):\n",
    "    return str(string)[n - 1]\n",
    "def cpuUtilised(cluster, app_name):\n",
    "    output = os.popen('kubectl top pod --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    which_app = len(app_name)\n",
    "    cpu = ''\n",
    "    cpu_u=0\n",
    "    limits=0\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            if getchar(items[0], which_app) == 's':\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'm':\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'l':\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "\n",
    "        \n",
    "                \n",
    "#    print(limits - cpu_u)\n",
    "    return cpu_u\n",
    "\n",
    "\n",
    "cpuUtilised(\"cluster1-cntx\",\"firewall\"+'m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The available CPU of a microservice running on a cluster\n",
    "import os\n",
    "def getchar(string, n):\n",
    "    return str(string)[n - 1]\n",
    "def cpuAvail(cluster, app_name):\n",
    "    output = os.popen('kubectl top pod --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    which_app = len(app_name)\n",
    "    cpu = ''\n",
    "    cpu_u=0\n",
    "    limits=0\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            if getchar(items[0], which_app) == 's':\n",
    "                limits = 600\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'm':\n",
    "                limits = 5*600\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'l':\n",
    "                limits = 10*600\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "\n",
    "        \n",
    "                \n",
    "#    print(limits - cpu_u)\n",
    "    return limits - cpu_u\n",
    "\n",
    "\n",
    "cpuAvail(\"cluster1-cntx\",\"firewall\"+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which microservice configuration is running: small, medium, or large\n",
    "def getchar(string, n):\n",
    "    return str(string)[n - 1]\n",
    "def whichConf(cluster, app_name):\n",
    "    size = []\n",
    "    output = os.popen('sudo kubectl get services --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    which_app = len(app_name)\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            size.append(getchar(items[0], which_app+1))\n",
    "    #        print(size)\n",
    "    return size\n",
    "whichConf(\"cluster1-cntx\",\"firewall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function orders the running microservies on a cluster increasingly according to proccessing delay\n",
    "def orderService(cluster, app_name):\n",
    "    delay = []\n",
    "    size = whichConf(cluster, app_name)\n",
    "    for s in range(len(size)):\n",
    "        delay.append(bf[app_name][app_name+size[s]])\n",
    "    sorted_delay = [x for _,x in sorted(zip(delay,size))]\n",
    "    return sorted_delay\n",
    " #   print(delay)\n",
    "                \n",
    "    \n",
    "orderService(\"cluster1-cntx\",\"firewall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def getIP(cluster, app_name):\n",
    "    output = os.popen('kubectl get service -o wide --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    serviceIP = ''\n",
    "    port=''\n",
    "    which_app = len(app_name)\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            serviceIP=items[2]+\":\"+items[4][:4]\n",
    "    return serviceIP\n",
    "getIP(\"cluster1-cntx\",\"firewall\"+'m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster1-cntx', 'cluster2-cntx']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function orders the available links from source cluster increasingly according to latency\n",
    "def orderlink(cluster):\n",
    "    delay = []\n",
    "    cnx = []\n",
    "    for key in range(len(list(link_latency[\"cluster1-cntx\"].keys()))):\n",
    "        delay.append(link_latency[\"cluster1-cntx\"][list(link_latency[\"cluster1-cntx\"].keys())[key]])\n",
    "        cnx.append(list(link_latency[\"cluster1-cntx\"].keys())[key])\n",
    "    sorted_delay = [x for _,x in sorted(zip(delay,cnx))]\n",
    "    return sorted_delay\n",
    " #   print(delay)\n",
    "                \n",
    "    \n",
    "orderlink(\"cluster1-cntx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3771"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Available CPU on a cluster in millicores\n",
    "def clusterCPU(cluster):\n",
    "    cpu=0\n",
    "    cpu_cl=0\n",
    "    nodes=0\n",
    "    output = os.popen('sudo kubectl top nodes --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0] != \"NAME\":\n",
    "            cpu = items[1]\n",
    "            cpu = cpu[:-1]\n",
    "            cpu = int(cpu)\n",
    "            cpu_cl = cpu_cl + cpu\n",
    "            nodes=nodes+1\n",
    "    return nodes*2000 - cpu_cl\n",
    "\n",
    "clusterCPU('cluster1-cntx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Place and Assign new mircroservice to requests</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>Place and Assign new mircroservice to requests</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster1-cntx',\n",
       " 'cluster1-cntx',\n",
       " 'cluster1-cntx',\n",
       " 'cluster2-cntx',\n",
       " 'cluster2-cntx',\n",
       " 'cluster2-cntx']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns the lists of clusters on which a microservice type is running\n",
    "def checkdeploy(app_name):\n",
    "    cl = 0\n",
    "    p=0\n",
    "    cluster=[]\n",
    "    while p == 0:\n",
    "        for cl in range(len(G)):\n",
    "            i = list(G.keys())[cl]\n",
    "            output = os.popen('sudo kubectl get services --context=' +i).read()\n",
    "            lines = output.split(\"\\n\")\n",
    "            which_app = len(app_name)\n",
    "            for line in lines[:-1]:\n",
    "                items = line.split()\n",
    "                if items[0][:which_app] == app_name:\n",
    "                    cluster.append(i)\n",
    "                    p=1\n",
    "                    \n",
    "    return cluster\n",
    "checkdeploy(\"proxy\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignService(i, q, usecase_s):\n",
    "#    req = 1\n",
    "#    cl = 1\n",
    "    config_idx='' \n",
    "    size=[]\n",
    "    avail=0\n",
    "    cpu_q= usecase_s[u][q]\n",
    "    r = runningPods(i, q)\n",
    "    if r >= 1: #there's a function already running on this cluster \n",
    "        size = orderService(i,q) #which function size is running? order by delay\n",
    "        for s in range(len(size)): \n",
    "            avail = cpuAvail(i, q+size[s]) #the available CPU depends on the service configuration\n",
    "            if avail > cpu_q: #if the available resources > profiled resources requested \n",
    "                config_idx = size[s] #then get IP to assign the service to proxy\n",
    "                break\n",
    "    elif r ==0 : #no service running \n",
    "        config_idx=''          \n",
    "    #print(proxy_conf)\n",
    "    return config_idx\n",
    "\n",
    "def newService(i,q, usecase_s):\n",
    "    config_idx=''\n",
    "    cpu_q= usecase_s[u][q]\n",
    "    r = runningPods(i, q)\n",
    "    if 600 > cpu_q and runningPods(i, q+'s') == 0 and 600 < clusterCPU(i): #deploy new small service\n",
    "        #place(i,q,'s')\n",
    "        config_idx='s'       \n",
    "    elif 5*600 > cpu_q and runningPods(i, q+'m')==0 and 5*600 < clusterCPU(i): #deploy new medium service\n",
    "        config_idx='m'\n",
    "    elif 10*600 > cpu_q and runningPods(i, q+'l')==0 and 10*600 < clusterCPU(i): #deploy new large service\n",
    "        config_idx='l'\n",
    "\n",
    "  #  elif proxy_conf=='':\n",
    "  #      print(\"placement failed\")\n",
    "        \n",
    "    \n",
    "    #print(proxy_conf)\n",
    "    return config_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Monitor and delete function</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>Monitor and delete function</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def monitorUsage(t): #This function is terminate services once idle for a time \n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def deletePod(app_name, timer, cluster):\n",
    "    #print(\"thread is starting\")\n",
    "    action=[None] * 5\n",
    "    policy=[]\n",
    "    cpu = 0\n",
    "    timeout = time.time() + timer\n",
    "    while cpu == 0:\n",
    "        cpu= cpuUtilised(cluster, app_name)\n",
    "        if time.time() > timeout:\n",
    "            action[0]=cluster\n",
    "            action[1]=0\n",
    "            action[2]= app_name[len(app_name)-1]\n",
    "            action[3]= app_name[:-1]\n",
    "            action[4]= 0\n",
    "            policy.append(action)\n",
    "            break\n",
    "    return policy\n",
    "class ThreadWithResult(threading.Thread):\n",
    "    def __init__(self, group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None):\n",
    "        def function():\n",
    "            self.result = target(*args, **kwargs)\n",
    "        super().__init__(group=group, target=function, name=name, daemon=daemon)\n",
    "\n",
    "        \n",
    "def monitorUsage(cluster, timer):\n",
    "\n",
    "    for service in range(len(bf)):\n",
    "        threads = list()\n",
    "        ser = list(bf.keys())[service]\n",
    "        s = whichConf(cluster,ser)\n",
    "        if s != []:\n",
    "            for size in range(len(s)):\n",
    "                app_name=ser+s[size]\n",
    "                x =ThreadWithResult(target=deletePod, args=(app_name, timer, cluster,))\n",
    "                x.daemon = True\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "            for x in threads:\n",
    "                x.join()\n",
    "                print(x.result)\n",
    "                \n",
    "\n",
    "        \n",
    "#monitorUsage(\"cluster1-cntx\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With new requests you need to initialise and empty the these values again\n",
    "#Run this once with every new G_req\n",
    "\n",
    "m_ser = []\n",
    "m_cl= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateAssign(a_ser):\n",
    "    m_ser.append(a_ser)\n",
    "    return  m_ser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(m_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firewall']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['cluster1-cntx', 1, 's', 'firewall', 'proxy']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this feeds the RL agent with heuristic policies \n",
    "#[cluster-idx, action-idx, config-idx, app-idx, proxy-idx]\n",
    "\n",
    "def main(proxy, G_req):\n",
    "\n",
    "    proxy_idx=proxy\n",
    "    action=[None] * 5\n",
    "    policy=[]\n",
    "    global m_cl\n",
    "    global m_ser\n",
    "\n",
    "\n",
    "    startingCluster = checkdeploy(proxy_idx)\n",
    "    OG = orderlink(startingCluster) #try the fastest link first\n",
    "    for cl in range(len(OG)): #go over clusters \n",
    "        i = list(G.keys())[cl]\n",
    "        for req in range(len(G_req)):\n",
    "            q = list(G_req.keys())[req] #go over functions to deploy\n",
    "            if q not in m_ser:\n",
    "                if q == list(G_req.keys())[0]: #The first function can be deployed on any cluster\n",
    "                    if assignService(i,q, usecase) != \"\": #there is atleast one good candidate service running on i \n",
    "                        action[0]=i\n",
    "                        action[1]=1\n",
    "                        action[2]= assignService(i,q, usecase)\n",
    "                        action[3]= q\n",
    "                        action[4]= proxy_idx\n",
    "                        policy.append(action)\n",
    "                        m_cl= i #to check connection with next deployment\n",
    "                        #m_serv=m_ser.append(q)\n",
    "                        updateAssign(q)\n",
    "                        #print(m_ser)\n",
    "                        return policy\n",
    "                elif m_cl in G[i]: #connection with previous bf and not already deployed\n",
    "                    if assignService(i,q, usecase) != \"\": #can be deployed\n",
    "                        action[0]=i\n",
    "                        action[1]=1\n",
    "                        action[2]= assignService(i,q, usecase)\n",
    "                        action[3]= q\n",
    "                        action[4]= proxy_idx\n",
    "                        policy.append(action)\n",
    "                        m_cl= i\n",
    "                        #m_ser=m_ser.append(q)\n",
    "                        updateAssign(q)\n",
    "                        #print(m_ser)\n",
    "                        return policy\n",
    "            else: \n",
    "                continue\n",
    "        if len(m_ser) < len(G_req): #not all deployed yet we try a different cluster      \n",
    "            continue \n",
    "        else:\n",
    "            break\n",
    "    if len(m_ser) < len(G_req): #not all functions could be deployed \n",
    "        for cl in range(len(OG)): #go over clusters \n",
    "            i = list(G.keys())[cl]\n",
    "            for req in range(len(G_req)):\n",
    "                q = list(G_req.keys())[req] #go over functions to deploy\n",
    "                if q not in m_ser:\n",
    "                    if q == list(G_req.keys())[0]:\n",
    "                        if newService(i,q, usecase) != '':\n",
    "                            action[0]=i\n",
    "                            action[1]=2\n",
    "                            action[2]= newService(i,q, usecase)\n",
    "                            action[3]= q\n",
    "                            action[4]= proxy_idx\n",
    "                            policy.append(action)\n",
    "                            m_cl= i\n",
    "                            #m_ser=m_ser.append(q)\n",
    "                            updateAssign(q)\n",
    "                            #print(m_ser)\n",
    "                            return policy\n",
    "                    elif m_cl in G[i]: #connection with previous bf and not already deployed\n",
    "                        if newService(i,q, usecase) != '':\n",
    "                            action[0]=i\n",
    "                            action[1]=2\n",
    "                            action[2]= newService(i,q, usecase)\n",
    "                            action[3]= q\n",
    "                            action[4]= proxy_idx\n",
    "                            policy.append(action)\n",
    "                            m_cl= i\n",
    "                            #m_ser=m_ser.append(q)\n",
    "                            updateAssign(q)\n",
    "                            #print(m_ser)\n",
    "                            return policy\n",
    "            if len(m_ser) < len(G_req): #not all deployed yet we try a different cluster      \n",
    "                continue \n",
    "            else:\n",
    "                break\n",
    "    #proxyConfig(IPs, proxy)\n",
    "    #print(IPs)\n",
    "    #print(m_ser)\n",
    "    \n",
    "    \n",
    "#main(\"proxy\", G_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Deploy and monitor microservices</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>Deploy and monitor microservices</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def deploy(proxy, G_req):\n",
    "#    for r in range(len(G_req)):\n",
    "#        main(proxy, G_req)\n",
    "#        time.sleep(1)\n",
    "\n",
    "def heuristic():\n",
    "    threads = list()\n",
    "    x =ThreadWithResult(target=main, args=(\"proxy\", G_req, ))\n",
    "    x.daemon = True\n",
    "    x.start()\n",
    "    threads.append(x)\n",
    "    for cl in range(len(G)): #go over clusters \n",
    "        i = list(G.keys())[cl]\n",
    "        x = ThreadWithResult(target=monitorUsage, args=(i,10000, ))\n",
    "        x.daemon = True\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "    for x in threads:\n",
    "        x.join()\n",
    "        print(x.result)\n",
    "        \n",
    "heuristic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
